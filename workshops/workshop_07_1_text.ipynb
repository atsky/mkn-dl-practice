{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata torchtext"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqq2FZvK-mpn",
        "outputId": "614c091e-169e-4e4e-b0dc-ac7ae80c1030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchdata\n",
            "  Downloading torchdata-0.4.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.13.1)\n",
            "Collecting urllib3>=1.25\n",
            "  Downloading urllib3-1.26.12-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 51.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch==1.12.1 in /usr/local/lib/python3.7/dist-packages (from torchdata) (1.12.1+cu113)\n",
            "Collecting portalocker>=2.0.0\n",
            "  Downloading portalocker-2.6.0-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchdata) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.12.1->torchdata) (4.1.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.64.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.21.6)\n",
            "Collecting urllib3>=1.25\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 50.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchdata) (2022.9.24)\n",
            "Installing collected packages: urllib3, portalocker, torchdata\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed portalocker-2.6.0 torchdata-0.4.1 urllib3-1.25.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kp5bBv9EpgAt"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchtext.datasets import AG_NEWS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeU8Y7vOp_LU"
      },
      "source": [
        "Для начала сделаем на торче что-нибудь нлп-шное.\n",
        "\n",
        "\n",
        "Начнём со стандартного тьюториала по классификации текстов с помощью torchtext."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIIxOvyRtp6b"
      },
      "source": [
        "## Описание датасета\n",
        "\n",
        "*AG is a collection of more than 1 million news articles. News articles have been gathered from more than 2000 news sources by ComeToMyHead in more than 1 year of activity. ComeToMyHead is an academic news search engine which has been running since July, 2004. The dataset is provided by the academic comunity for research purposes in data mining (clustering, classification, etc), information retrieval (ranking, search, etc), xml, data compression, data streaming, and any other non-commercial activity. For more information, please refer to the link http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html .*\n",
        "\n",
        "*The AG's news topic classification dataset is constructed by Xiang Zhang (xiang.zhang@nyu.edu) from the dataset above. It is used as a text classification benchmark in the following paper: Xiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems 28 (NIPS 2015).*\n",
        "\n",
        "*The AG's news topic classification dataset is constructed by choosing 4 largest classes from the original corpus. Each class contains 30,000 training samples and 1,900 testing samples. The total number of training samples is 120,000 and testing 7,600.*\n",
        "\n",
        "[ag subset tf](https://www.tensorflow.org/datasets/catalog/ag_news_subset)\n",
        "\n",
        "in `torchtext`:\n",
        "\n",
        "    train: 120000\n",
        "    test: 7600\n",
        "\n",
        "https://pytorch.org/text/stable/datasets.html#ag-news\n",
        "\n",
        "\n",
        "    1 : World\n",
        "    2 : Sports\n",
        "    3 : Business\n",
        "    4 : Sci/Tec\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yn9ATxqupqim"
      },
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "train_iter = AG_NEWS(split='train')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "    for _, text in data_iter:\n",
        "        yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0w9d8RVq5g7"
      },
      "source": [
        "А что за данные-то?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHwGPo2AqS_-",
        "outputId": "8d0d6893-3fd3-4a61-9f6e-a18affb3de9f"
      },
      "source": [
        "# задаём снова, так как мы уже его прочитали, когда строили словарь\n",
        "train_iter = AG_NEWS(split='train')\n",
        "\n",
        "for item in train_iter:\n",
        "  print(item)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_Q0Rl_XrWdW"
      },
      "source": [
        "Что делает словарь"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WlxZKzKqe9O",
        "outputId": "d3472146-b02c-4f1c-b45a-e4cfa5646761"
      },
      "source": [
        "vocab([\"so\", \"call\", \"me\", \"maybe\", \"sdfsgsdgsdf\", \"<unk>\"])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[303, 683, 2082, 3063, 0, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9YLEKLurKBm"
      },
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhBGYGfkrrPh"
      },
      "source": [
        "Теперь будем иметь дело с чем-то уже нам знакомым"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjzIMYQ0ruWp",
        "outputId": "47604acc-f692-426d-d1bf-b646c204e360"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYt7nA0isR7w"
      },
      "source": [
        "Даталоадер будет зачитывать тексты, а мы их будем преобразовывать в вид, пригодный для обработки нейронными сетями, для этого нам придётся задать специальную функцию `collate_fn`.\n",
        "\n",
        "    Collate = collect and combine (texts, information, or sets of figures) in proper order\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9RbwGMtVrvSp",
        "outputId": "808af8a7-a114-43cd-cc07-e3ae81fa7b86"
      },
      "source": [
        "def collate_batch(batch):\n",
        "  \n",
        "  label_list, text_list, offsets = [], [], [0]\n",
        "  \n",
        "  for _label, _text in batch:\n",
        "    # меняем метки на индексы\n",
        "    label_list.append(label_pipeline(_label))\n",
        "    # меняем токены на индексы\n",
        "    processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "    text_list.append(processed_text)\n",
        "\n",
        "    # сохраняем длину текста\n",
        "    offsets.append(processed_text.size(0))\n",
        "\n",
        "  label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "  offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "\n",
        "  # объединяем список тензоров в один вдоль нулевого измерения\n",
        "  text_list = torch.cat(text_list)\n",
        "  return label_list.to(device), text_list.to(device), offsets.to(device)\n",
        "\n",
        "train_iter = AG_NEWS(split='train')\n",
        "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "len(list(dataloader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15000"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEhE5DshvXV9"
      },
      "source": [
        "## Модель\n",
        "\n",
        "Для каждого токена будем брать эмбеддинг, и усреднять вдоль всего текста. На основе получившегося вектора будем делать предсказание.\n",
        "\n",
        "Для таких подходов есть специальный класс, который называется [EmbeddingBag](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html#torch.nn.EmbeddingBag).\n",
        "\n",
        "    Computes sums or means of ‘bags’ of embeddings, without instantiating the intermediate embeddings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjJB210IuPMl"
      },
      "source": [
        "class TextClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super(TextClassificationModel, self).__init__()        \n",
        "        # вот наш усреднитель\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, \n",
        "                                         sparse=True, mode=\"mean\")\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmEirbdiwjKf"
      },
      "source": [
        "## Собственно, обучение"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXjPfbZfwHwn"
      },
      "source": [
        "# снова сбрасываем итератор в нулевую позицию\n",
        "train_iter = AG_NEWS(split='train')\n",
        "\n",
        "# число классов\n",
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "\n",
        "# размер словаря\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# размер эмбеддинга\n",
        "emsize = 64\n",
        "\n",
        "# ну тут всё понятно и знакомо\n",
        "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YJ7NZrQw3d6"
      },
      "source": [
        "import time\n",
        "\n",
        "def train(dataloader, criterion):\n",
        "  \n",
        "  # режим обучения\n",
        "  model.train()\n",
        "\n",
        "  # метрики-шметрики\n",
        "  total_acc, total_count = 0, 0\n",
        "\n",
        "  # как часто писать отчёты в консоль\n",
        "  log_interval = 500\n",
        "  start_time = time.time()\n",
        "\n",
        "  for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    predicted_label = model(text, offsets)\n",
        "    loss = criterion(predicted_label, label)\n",
        "    loss.backward()\n",
        "    \n",
        "    # обрезка градиентов\n",
        "    # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "    optimizer.step()\n",
        "    \n",
        "    total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "    total_count += label.size(0)\n",
        "    \n",
        "    if idx % log_interval == 0 and idx > 0:\n",
        "      elapsed = time.time() - start_time\n",
        "      print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "            '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                        total_acc/total_count))\n",
        "      total_acc, total_count = 0, 0\n",
        "      start_time = time.time()\n",
        "\n",
        "def evaluate(dataloader, criterion):\n",
        "\n",
        "  model.eval()\n",
        "  total_acc, total_count = 0, 0\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "      predicted_label = model(text, offsets)\n",
        "      loss = criterion(predicted_label, label)\n",
        "      total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
        "      total_count += label.size(0)\n",
        "\n",
        "  return total_acc/total_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaR4HPC2zMCL"
      },
      "source": [
        "## Пора."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnXbZfJ7zKnk",
        "outputId": "2ddb51ed-7250-46db-c342-5d4c61603878"
      },
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "\n",
        "\n",
        "EPOCHS = 10 # epoch\n",
        "LR = 0.5  # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "\n",
        "# тут логарифм+софтмакс происходит под капотом\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "\n",
        "# понижает LR каждой группы параметров на gamma\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "total_accu = None\n",
        "\n",
        "# зачитаем и трейн и тест (точней, берём итераторы)\n",
        "train_iter, test_iter = AG_NEWS()\n",
        "\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "\n",
        "# сколько берём для трейна, остальное пойдёт на валидацию\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader, criterion)\n",
        "    accu_val = evaluate(valid_dataloader, criterion)\n",
        "\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   500/ 1782 batches | accuracy    0.405\n",
            "| epoch   1 |  1000/ 1782 batches | accuracy    0.597\n",
            "| epoch   1 |  1500/ 1782 batches | accuracy    0.714\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 14.08s | valid accuracy    0.775 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |   500/ 1782 batches | accuracy    0.801\n",
            "| epoch   2 |  1000/ 1782 batches | accuracy    0.823\n",
            "| epoch   2 |  1500/ 1782 batches | accuracy    0.841\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 13.48s | valid accuracy    0.848 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |   500/ 1782 batches | accuracy    0.860\n",
            "| epoch   3 |  1000/ 1782 batches | accuracy    0.865\n",
            "| epoch   3 |  1500/ 1782 batches | accuracy    0.870\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 13.36s | valid accuracy    0.868 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |   500/ 1782 batches | accuracy    0.880\n",
            "| epoch   4 |  1000/ 1782 batches | accuracy    0.881\n",
            "| epoch   4 |  1500/ 1782 batches | accuracy    0.887\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 13.65s | valid accuracy    0.876 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |   500/ 1782 batches | accuracy    0.892\n",
            "| epoch   5 |  1000/ 1782 batches | accuracy    0.892\n",
            "| epoch   5 |  1500/ 1782 batches | accuracy    0.893\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 13.28s | valid accuracy    0.881 \n",
            "-----------------------------------------------------------\n",
            "| epoch   6 |   500/ 1782 batches | accuracy    0.899\n",
            "| epoch   6 |  1000/ 1782 batches | accuracy    0.901\n",
            "| epoch   6 |  1500/ 1782 batches | accuracy    0.898\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time: 13.65s | valid accuracy    0.886 \n",
            "-----------------------------------------------------------\n",
            "| epoch   7 |   500/ 1782 batches | accuracy    0.905\n",
            "| epoch   7 |  1000/ 1782 batches | accuracy    0.905\n",
            "| epoch   7 |  1500/ 1782 batches | accuracy    0.904\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time: 13.39s | valid accuracy    0.888 \n",
            "-----------------------------------------------------------\n",
            "| epoch   8 |   500/ 1782 batches | accuracy    0.910\n",
            "| epoch   8 |  1000/ 1782 batches | accuracy    0.907\n",
            "| epoch   8 |  1500/ 1782 batches | accuracy    0.908\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time: 13.48s | valid accuracy    0.891 \n",
            "-----------------------------------------------------------\n",
            "| epoch   9 |   500/ 1782 batches | accuracy    0.913\n",
            "| epoch   9 |  1000/ 1782 batches | accuracy    0.911\n",
            "| epoch   9 |  1500/ 1782 batches | accuracy    0.913\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time: 13.44s | valid accuracy    0.893 \n",
            "-----------------------------------------------------------\n",
            "| epoch  10 |   500/ 1782 batches | accuracy    0.916\n",
            "| epoch  10 |  1000/ 1782 batches | accuracy    0.915\n",
            "| epoch  10 |  1500/ 1782 batches | accuracy    0.914\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time: 13.33s | valid accuracy    0.895 \n",
            "-----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXdP_-bn7e31",
        "outputId": "c1f39ade-5250-411e-b5f4-18c3b084075f"
      },
      "source": [
        "print('Checking the results of test dataset.')\n",
        "accu_test = evaluate(test_dataloader, criterion)\n",
        "print('test accuracy {:8.3f}'.format(accu_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking the results of test dataset.\n",
            "test accuracy    0.899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mQItgF698Ta7"
      },
      "source": [
        "### А как это использовать? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Iyl74dJ8Jb7"
      },
      "source": [
        "ag_news_label = {1: \"World\",\n",
        "                 2: \"Sports\",\n",
        "                 3: \"Business\",\n",
        "                 4: \"Sci/Tec\"}\n",
        "\n",
        "def predict(text, text_pipeline):\n",
        "    with torch.no_grad():\n",
        "        text = torch.tensor(text_pipeline(text))\n",
        "        output = model(text, torch.tensor([0]))\n",
        "        return ag_news_label[output.argmax(1).item() + 1]\n",
        "        \n",
        "model = model.to(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "DrMOcv7G9J8D",
        "outputId": "6ca30abb-de5a-4e04-e3df-fda1584b0e00"
      },
      "source": [
        "predict('The release of the Squid game series.', text_pipeline=text_pipeline)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sports'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4qj2n4o219V"
      },
      "source": [
        "# Задания\n",
        "\n",
        "1. Попробуйте другие варианты EmbeddingBag.\n",
        "2. Рассмотрите другие варианты schedulers. Как они влияют на скорость обучения?\n",
        "3. Скачайте [GoogleNews-vectors-negative300.bin.gz](https://code.google.com/archive/p/word2vec/). Используя слой [Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) и [советы](https://stackoverflow.com/a/63074440/1616037), проинициализируйте им матрицу векторных представлений. Попробуйте решить эту задачу. Лучше ли стали результаты?\n"
      ]
    }
  ]
}